{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1JkMdMJaN1FVtR6eDBnP1-ow0eHO6-6VQ","authorship_tag":"ABX9TyNLys0TG2NYetTtQKU9ybNg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZycEp0PZCAIS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749378706027,"user_tz":-420,"elapsed":3442,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}},"outputId":"3e70ffc1-bf3e-4fb8-fa95-57b877e744ea"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SiKUEslkB1mk","executionInfo":{"status":"ok","timestamp":1749390395532,"user_tz":-420,"elapsed":4929,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}},"outputId":"98ff70ef-16a3-422c-dcc0-869167c02e25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/face-mask-dataset\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"omkargurav/face-mask-dataset\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["import os\n","\n","# Print file/folder list\n","print(os.listdir(path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6JpyyrVGW5c","executionInfo":{"status":"ok","timestamp":1749390398247,"user_tz":-420,"elapsed":29,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}},"outputId":"e33ae372-e2d8-4a9c-d0dc-a58070e63eae"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['data']\n"]}]},{"cell_type":"code","source":["import zipfile\n","\n","# Check if zip file exists\n","zip_path = os.path.join(path, \"face-mask-dataset.zip\")  # Adjust the file name if necessary\n","if os.path.exists(zip_path):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(path)\n","    print(\"Dataset has been unzip!\")\n","    print(os.listdir(path))  # Check the content again after unzipping"],"metadata":{"id":"7n8gEfP5GauD","executionInfo":{"status":"ok","timestamp":1749390399696,"user_tz":-420,"elapsed":10,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import os\n","\n","# Suppose the folders containing the images are 'data/with_mask' and 'data/without_mask'\n","with_mask_dir = os.path.join(path, 'data/with_mask')  # Adjust to actual pathế\n","without_mask_dir = os.path.join(path, 'data/without_mask')\n","\n","# Load a sample image\n","sample_image_path = os.path.join(with_mask_dir, os.listdir(with_mask_dir)[0])\n","img = cv2.imread(sample_image_path)\n","print(\"Sample image size:\", img.shape)\n","\n","# List the number of images in each folder\n","print(\"Number of photos wearing masks:\", len(os.listdir(with_mask_dir)))\n","print(\"Number of photos without masks:\", len(os.listdir(without_mask_dir)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OY5pGHw9Gesr","executionInfo":{"status":"ok","timestamp":1749390403102,"user_tz":-420,"elapsed":645,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}},"outputId":"5c2fc335-ec14-436a-83e2-4438825fc065"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample image size: (800, 800, 3)\n","Number of photos wearing masks: 3725\n","Number of photos without masks: 3828\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nC7qyKeVxzbP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","import time\n","\n","# Thiết lập các tham số\n","# data_dir = 'dataset'      # đường dẫn đến folder dataset\n","data_dir = os.path.join(path, 'data')\n","num_classes = 2           # Có 2 lớp: with_mask và without_mask\n","batch_size = 32\n","num_epochs = 10\n","learning_rate = 0.001\n","input_size = 128\n","\n","# Sử dụng GPU nếu có\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Thiết bị sử dụng: {device}\")\n","\n","# Định nghĩa chuyển đổi dữ liệu: resize, crop, normalization, …\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((input_size, input_size)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],  # giá trị chuẩn cho ImageNet\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((input_size, input_size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","# Tạo tập dữ liệu với 80% train và 20% validation\n","full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n","dataset_size = len(full_dataset)\n","indices = list(range(dataset_size))\n","split = int(0.2 * dataset_size)\n","\n","# Khuynh hướng trộn dữ liệu\n","import numpy as np\n","np.random.shuffle(indices)\n","train_idx, val_idx = indices[split:], indices[:split]\n","\n","from torch.utils.data.sampler import SubsetRandomSampler\n","train_sampler = SubsetRandomSampler(train_idx)\n","val_sampler = SubsetRandomSampler(val_idx)\n","\n","train_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n","val_loader = DataLoader(full_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=4)\n","\n","dataloaders = {'train': train_loader, 'val': val_loader}\n","dataset_sizes = {'train': len(train_idx), 'val': len(val_idx)}\n","\n","# Định nghĩa một kiến trúc CNN đơn giản (hoặc dùng transfer learning nếu muốn)\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super(SimpleCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            # Lớp 1\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            # Lớp 2\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            # Lớp 3\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        # Tính toán kích thước feature sau khi pooling 3 lần:\n","        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(128 * 4 * 4, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","# Khởi tạo mô hình và chuyển sang device\n","model = SimpleCNN(num_classes=num_classes)\n","model = model.to(device)\n","\n","# Định nghĩa loss function và optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Hàm huấn luyện\n","def train_model(model, criterion, optimizer, num_epochs=10):\n","    best_model_wts = model.state_dict()\n","    best_acc = 0.0\n","    since = time.time()\n","\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print('-'*10)\n","\n","        # Mỗi epoch có 2 giai đoạn: train và validation\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","            else:\n","                model.eval()\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Duyệt qua dữ liệu\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                # forward propagation\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward propagation nếu đang ở giai đoạn train\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","\n","            # Lưu mô hình tốt nhất\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = model.state_dict()\n","\n","        print()\n","    time_elapsed = time.time() - since\n","    print(f'Huấn luyện hoàn tất trong {time_elapsed//60:.0f} phút {time_elapsed%60:.0f} giây')\n","    print(f'Accuracy tốt nhất: {best_acc:.4f}')\n","\n","    # Tải trọng số tốt nhất\n","    model.load_state_dict(best_model_wts)\n","    return model\n","\n","# Huấn luyện mô hình\n","model = train_model(model, criterion, optimizer, num_epochs=num_epochs)\n","\n","# Lưu mô hình đã huấn luyện\n","torch.save(model.state_dict(), '/content/drive/MyDrive/Project/Face Mask Detection/model/face_mask_detector.pth')\n","print(\"Mô hình đã được lưu vào face_mask_detector.pth\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4hQY1HMxzVg","executionInfo":{"status":"ok","timestamp":1749390711773,"user_tz":-420,"elapsed":223975,"user":{"displayName":"Trúc Đỗ","userId":"03126312965648259752"}},"outputId":"ed46a2d6-2b24-4e9c-86f0-1dacb36b3c58"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Thiết bị sử dụng: cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.3235 Acc: 0.8636\n","val Loss: 0.3059 Acc: 0.8662\n","\n","Epoch 2/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.2323 Acc: 0.9034\n","val Loss: 0.2467 Acc: 0.8921\n","\n","Epoch 3/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.2021 Acc: 0.9202\n","val Loss: 0.2058 Acc: 0.9166\n","\n","Epoch 4/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1725 Acc: 0.9322\n","val Loss: 0.1971 Acc: 0.9238\n","\n","Epoch 5/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1552 Acc: 0.9401\n","val Loss: 0.1607 Acc: 0.9437\n","\n","Epoch 6/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1387 Acc: 0.9444\n","val Loss: 0.2319 Acc: 0.9132\n","\n","Epoch 7/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1279 Acc: 0.9552\n","val Loss: 0.1313 Acc: 0.9510\n","\n","Epoch 8/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1065 Acc: 0.9583\n","val Loss: 0.1263 Acc: 0.9583\n","\n","Epoch 9/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.1067 Acc: 0.9605\n","val Loss: 0.2565 Acc: 0.8907\n","\n","Epoch 10/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["train Loss: 0.0945 Acc: 0.9654\n","val Loss: 0.1211 Acc: 0.9589\n","\n","Huấn luyện hoàn tất trong 3 phút 34 giây\n","Accuracy tốt nhất: 0.9589\n","Mô hình đã được lưu vào face_mask_detector.pth\n"]}]}]}